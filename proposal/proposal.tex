\documentclass[12pt,leqno,twoside]{article}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage{natbib}
\setlength{\parskip}{1.2ex}        % space between paragraphs
\setlength{\parindent}{2em}        % amount of indention
\setlength{\textwidth}{7truein}      % default = 6.5"
\setlength{\oddsidemargin}{-12mm}   % default = 0"
\setlength{\evensidemargin}{-12mm}   % default = 0"
\setlength{\textheight}{225mm}     % default = 9"
\setlength{\topmargin}{-12mm}      % default = 0"
\usepackage[all]{xy}
\usepackage{tipa}
\input xy
\xyoption{all}
\usepackage{listings}
\newcommand{\indicator}[1]{\mathbbm{1}{\left[ {#1} \right] }}
\lstdefinelanguage{Scala}%
{morekeywords={abstract,case,catch,char,class,%
def,else,extends,final,%
if,import,%
match,module,new,null,object,override,package,private,protected,%
public,return,super,this,throw,trait,try,val,var,with%
},%
sensitive,%
morecomment=[l]//,%
morecomment=[s]{/*}{*/},%
morestring=[b]``,%
morestring=[b]',%
showstringspaces=false%
}[keywords,comments,strings]%

\lstset{language=Scala,%
mathescape=true,%
columns=[c]fixed,%
basewidth={0.5em, 0.40em},%
basicstyle=\tt,%
keywordstyle=\bfseries,%
}

\title{Project Proposal}
\author{David Hall \and Alex Kantchelian}
\begin{document}
\maketitle

\section{Introduction}

We propose to study the effect of explicitly fault-prone systems
on machine learning workloads. We in particular plan to focus on the
optimization of convex objective functions commonly used in machine
learning using stochastic gradient descent (SGD). Specifically, we
will introduce plausible changes into a standard architecture that
relax some of the usual correctness guarantees of the processor.
Moreover, we will do this while trying to preserve correctness
guarantees and convergence rates for the overall algorithm. 

We are particularly interested in whether or not commonplace
protections against read-after-write, write-after-read, and
write-after-write hazards can be at least partially relaxed.
Protection against these kinds of hazards is crucial for applications
that are not tolerant to hardware error. However, allowing these
hazards to happen occasionally may not be fatal to a fault-tolerant
machine learning algorithm. Moreover, by allowing for machines
that make errors, we might be able to find architectures that are
faster or more efficient at achieving the same level of precision.

%SGD is fast becoming the algorithm of choice for scalable machine
%learning particularly in distributed settings (e.g.
%\citet{agarwal11reliable}).

\section{Intuitions and Background}

Before going further, we quickly review the basic stochastic gradient
algorithm. Given a (sub)differentiable function $f$, SGD seeks to
optimize \begin{equation}
  \begin{split}
    \min_{x \in \mathcal{X}} f(x) + \phi(x)
   \end{split}
 \end{equation}
where $f(x)$ is usually called a ``loss'' function.  Typically in
the literature, we are only presented with successive noisy
approximations to $f$ (which we label $\tilde f$), usually arising
from the data itself.  Therefore, we introduce  $\phi(x)$ as a
regularization function that intuitively ``smooths'' $x$, making
it more resistant to noise in the data.

To actually optimize this function, we take small steps along the 
approximation to the gradient:
\begin{equation}
  \begin{split}
    x_{t+1} \gets x_t - \alpha_t \left ( \nabla \tilde f(x) + \nabla \phi(x)\right )
   \end{split}
 \end{equation}
$\alpha_t$ is a series of step sizes. Because the
steps are small, and because the overall objective function is
well-behaved, SGD is guaranteed to converge to the optimum, even
in the presence of noise in the training data. We believe this
noise-tolerance can be used to guarantee convergence under mild to
moderate levels of error, even at the hardware level.

\section{Previous work}

There has been a recent uptick of interest in the area of optimization
under error-prone conditions, both from the machine learning community
and the systems community. 

On the systems side, \citet{sartori11stochastic} argued empirically
for what they term ``stochastic processors,'' which are classical
processors that have been designed with the assumption that a certain
amount of imprecision is acceptable. In particular, they provide
experimental evidence that a variety of algorithms---including
certain varieties of SGD---can withstand random bit flips in the
floating point units. Moreover, the distribution of errors they
induced was consistent with the distribution of errors observed
from voltage overscaling. Similarly, \citet{oberil11numerical}
investigated variations on the conjugate gradient algorithm that
were robust to random bit flips.

On the machine learning side, the most relevant work is
Hogwild!~\citep{niu11hogwild}. Essentially, the authors proposed
and analyzed a variant of parallelized SGD where no locking was
done in the updates to the weight vector $x$. This lock-free approach
does not suffer much degradation over locking SGD,  particularly
for sparse problems where the majority of the components of the
vector $x$ are not in use at any particular time. And, of course,
the resulting algorithm is much faster.  Interestingly, they are
still able to prove that their algorithm is guaranteed to converge
at rates that are essentially no different from ``correct'' SGD
algorithms.

\section{Approach}

Our proposed approach consists of three parts.  First, we will---in
a high-level language---simulate the effect of certain kinds of
hardware errors on the convergence rate and performance of different
variants of SGD for a few different machine learning workloads. For
instance, we might simulate a read-after-write hazard by randomly
stalling the actual commit of a value to the weight vector and
apply this model to a standard logistic regression or support vector
machine objective.

Second, depending on what kinds of hazards and what kinds of error
levels seem to be empirically feasible, we will investigate
modifications to a standard architecture that might increase its
error rate to allow the architecture to be faster or more power
efficient. We will verify our proposals empirically by modifying a
standard architecture in a simulator like Gem5~\citep{gem5}.

Finally, we will try to deploy the theoretical machinery from
\citet{niu11hogwild} or other related papers to prove that the
version of SGD we propose is indeed fault tolerant.

\bibliographystyle{plainnat}
\bibliography{refs} \end{document}
